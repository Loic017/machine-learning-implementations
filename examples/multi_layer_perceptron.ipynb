{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import loss_functions as ls\n",
    "import utils as ut\n",
    "from models import Model\n",
    "from layers import Linear, Flatten\n",
    "from activation_functions import Sigmoid, ReLU, Tanh\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (42000, 784), (42000,)\n",
      "Validation set: (9000, 784), (9000,)\n",
      "Test set: (9000, 784), (9000,)\n",
      "Class distribution in train set: Counter({np.int64(1): 4719, np.int64(7): 4385, np.int64(3): 4292, np.int64(2): 4171, np.int64(9): 4164, np.int64(0): 4146, np.int64(6): 4143, np.int64(8): 4096, np.int64(4): 4089, np.int64(5): 3795})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "\n",
    "# Define a transform to convert the data to tensors and normalize it\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "trainset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "concatset = ConcatDataset([trainset, testset])\n",
    "\n",
    "# Create a smaller balanced subset\n",
    "samples_per_class = 10000  # 500 samples per digit = 5000 total\n",
    "labels = torch.tensor([label for _, label in trainset])\n",
    "indices_by_label = {}\n",
    "\n",
    "# Group indices by label\n",
    "for i, label in enumerate(labels):\n",
    "    label_idx = label.item()\n",
    "    if label_idx not in indices_by_label:\n",
    "        indices_by_label[label_idx] = []\n",
    "    indices_by_label[label_idx].append(i)\n",
    "\n",
    "# Collect balanced subset indices\n",
    "subset_indices = []\n",
    "for label, indices in indices_by_label.items():\n",
    "    subset_indices.extend(indices[:samples_per_class])\n",
    "\n",
    "# Create the subset\n",
    "balanced_subset = Subset(trainset, subset_indices)\n",
    "\n",
    "# Extract data from the subset\n",
    "x, y = [], []\n",
    "for i in range(len(balanced_subset)):\n",
    "    data = balanced_subset[i]\n",
    "    x.append(data[0].numpy().flatten())\n",
    "    y.append(data[1])\n",
    "\n",
    "# Split into train/val/test\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "x_val = np.array(x_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"Train set: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {x_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {x_test.shape}, {y_test.shape}\")\n",
    "print(f\"Class distribution in train set: {Counter(y_train)}\")\n",
    "\n",
    "# Clean up\n",
    "del trainset\n",
    "del testset\n",
    "del balanced_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Layer (in 784, out 256), (activation relu)\n",
      "Linear Layer (in 256, out 10), (activation None)\n"
     ]
    }
   ],
   "source": [
    "model = Model(ls.multi_cross_entropy)\n",
    "model.add(Linear(28 * 28, 256, ReLU()))\n",
    "model.add(Linear(256, 10, None))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (42000, 784), Val shape: (9000, 784), Test shape: (9000, 784)\n",
      "Training epoch 0\n",
      "Epoch 0 loss -> 380.43445086449145\n",
      "Validation loss -> 81.74822977367633\n",
      "Training epoch 1\n",
      "Epoch 1 loss -> 378.91472165580586\n",
      "Validation loss -> 81.72693963673268\n",
      "Training epoch 2\n",
      "Epoch 2 loss -> 378.87361654541166\n",
      "Validation loss -> 81.42972852493388\n",
      "Training epoch 3\n",
      "Epoch 3 loss -> 378.91581451766524\n",
      "Validation loss -> 81.73908196161653\n",
      "Training epoch 4\n",
      "Epoch 4 loss -> 378.9276456396509\n",
      "Validation loss -> 81.76164968707792\n",
      "Training epoch 5\n",
      "Epoch 5 loss -> 378.8733027002945\n",
      "Validation loss -> 81.72292723393392\n",
      "Training epoch 6\n",
      "Epoch 6 loss -> 378.949467733321\n",
      "Validation loss -> 81.7901134645553\n",
      "Training epoch 7\n",
      "Epoch 7 loss -> 378.9319647778343\n",
      "Validation loss -> 81.78696064857444\n",
      "Training epoch 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_val\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Enlightenment/ml/neural-network-from-scratch/examples/../models.py:146\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, train_data, epochs, lr, validation_data, logging_predictions, test_set_logging)\u001b[0m\n\u001b[1;32m    143\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# FORWARD PASS -> Expects shape [batch_size, features]\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# If y is not the same shape as y_hat, convert it to one-hot encoding\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_hat\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape:\n",
      "File \u001b[0;32m~/Documents/Enlightenment/ml/neural-network-from-scratch/examples/../models.py:53\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x, loggings)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m     52\u001b[0m     assert_shape(arr\u001b[38;5;241m=\u001b[39mx, expected_shape\u001b[38;5;241m=\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], layer\u001b[38;5;241m.\u001b[39minput_size))\n\u001b[0;32m---> 53\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loggings:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Enlightenment/ml/neural-network-from-scratch/examples/../layers.py:51\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# print(f\"Weights shape {self.weights.shape}\")\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# print(f\"x shape {x.shape}\")\u001b[39;00m\n\u001b[1;32m     49\u001b[0m assert_shape(arr\u001b[38;5;241m=\u001b[39mx, expected_shape\u001b[38;5;241m=\u001b[39m(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size))\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m     52\u001b[0m assert_shape(arr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, expected_shape\u001b[38;5;241m=\u001b[39m(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Train shape: {x_train.shape}, Val shape: {x_val.shape}, Test shape: {x_test.shape}\"\n",
    ")\n",
    "\n",
    "loss = model.fit(\n",
    "    train_data=(x_train, y_train), validation_data=(x_val, y_val), epochs=20, lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss[\"train\"], label=\"Train Loss\")\n",
    "plt.plot(loss[\"val\"], label=\"Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Size: {len(x_test)}\")\n",
    "predictions = []\n",
    "for batch in x_test:\n",
    "    predictions.append(model.predict(batch))\n",
    "predictions = np.array(predictions)\n",
    "predictions = predictions.reshape((predictions.shape[0], -1))\n",
    "\n",
    "y_test = ut.one_hot_target(y_test, (y_test.shape[0], 10))\n",
    "print(predictions.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_idx = np.argmax(predictions, axis=1)\n",
    "y_test_idx = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.where(predictions_idx == y_test_idx, 1, 0)\n",
    "accuracy = np.sum(accuracy) / y_test_idx.shape[0] * 100\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
